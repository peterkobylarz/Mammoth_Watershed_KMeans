{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f56e38-662c-40d1-af49-b4f9ad46cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import earthaccess\n",
    "import earthpy as et\n",
    "import earthpy.earthexplorer as etee\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import gitpass\n",
    "import glob\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import rioxarray as rxr\n",
    "import rioxarray.merge as rxrm\n",
    "import shapely\n",
    "import xarray as xr\n",
    "import xrspatial\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data_dir = os.path.join(et.io.HOME, et.io.DATA_NAME)\n",
    "\n",
    "os.environ[\"GDAL_HTTP_MAX_RETRY\"] = \"5\"\n",
    "os.environ[\"GDAL_HTTP_RETRY_DELAY\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e109b-5e3d-497c-8e55-1cdcb58d5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching decorator\n",
    "\n",
    "def cached(key, override=False):\n",
    "    \"\"\"\"\"\"\n",
    "    def compute_and_cache_decorator(compute_function):\n",
    "        \"\"\"\"\"\"\n",
    "        def compute_and_cache(*args, **kwargs):\n",
    "            \"\"\"Perform a computation and cache, or load cached result\"\"\"\n",
    "            filename = os.path.join(et.io.HOME, et.io.DATA_NAME, 'jars', f'{key}.pickle')\n",
    "            \n",
    "            # Check if the cache exists already or override caching\n",
    "            if not os.path.exists(filename) or override:\n",
    "                # Make jars directory if needed\n",
    "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                \n",
    "                # Run the compute function as the user did\n",
    "                result = compute_function(*args, **kwargs)\n",
    "                \n",
    "                # Pickle the object\n",
    "                with open(filename, 'wb') as file:\n",
    "                    pickle.dump(result, file)\n",
    "            else:\n",
    "                # Unpickle the object\n",
    "                with open(filename, 'rb') as file:\n",
    "                    result = pickle.load(file)\n",
    "                    \n",
    "            return result\n",
    "        \n",
    "        return compute_and_cache\n",
    "    \n",
    "    return compute_and_cache_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536416ed-9adc-493c-a066-f320eb306225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download WBD spatial data\n",
    "\n",
    "wbd_path = os.path.join(\n",
    "    data_dir,\n",
    "    'earthpy-downloads',\n",
    "    'WBD_08_HU2_Shape',\n",
    "    'Shape',\n",
    "    'WBDHU12.shp'\n",
    ")\n",
    "\n",
    "wbd_url = (\"https://prd-tnm.s3.amazonaws.com/StagedProducts/\"\n",
    "           \"Hydrography/WBD/HU2/Shape/WBD_08_HU2_Shape.zip\"\n",
    "          )\n",
    "\n",
    "if not os.path.exists(wbd_path):\n",
    "    print('downloading ' + wbd_url)  \n",
    "    wbd_zip = et.data.get_data(url=wbd_url)\n",
    "else:\n",
    "    print(wbd_path + \" already exists\")\n",
    "\n",
    "# Create a GDF and plot the HUC    \n",
    "    \n",
    "wbd_gdf = gpd.read_file(wbd_path)\n",
    "huc_gdf = wbd_gdf[wbd_gdf['huc12'] == '080902030506']\n",
    "gv.tile_sources.EsriImagery() * gv.Polygons(huc_gdf).opts(\n",
    "    fill_alpha=0, line_color='blue', title='HUC 080902030506',\n",
    "    height=600, width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcf4a6e-4168-410d-a2fb-a0d4b609ab51",
   "metadata": {},
   "source": [
    "### Site Description\n",
    "\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a111f9-92ff-43aa-97b7-99481de4f9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search multispectral data from Earthdata\n",
    "\n",
    "earthaccess.login(persist=True)\n",
    "\n",
    "results = earthaccess.search_data(\n",
    "    short_name=\"HLSL30\",\n",
    "    cloud_hosted=True,\n",
    "    bounding_box=tuple(huc_gdf.total_bounds),\n",
    "    temporal=(\"2023-05-01\", \"2023-09-30\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73961d-754a-4a8c-9130-4ccfe167fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Earthaccess results so you only have to do it once\n",
    "@cached(\"open_results\", False)\n",
    "def open_results_and_cache(results):\n",
    "    open_results = earthaccess.open(results)\n",
    "    return open_results\n",
    "open_results = open_results_and_cache(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74f8a71-29f8-4c5d-8c93-1f61984d98ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the granules from Earthdata\n",
    "def create_granule_df(earthdata_results, open_results):\n",
    "    \"\"\"\n",
    "    Process the granules returned by an Earthdata search\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    earthdata_results : list\n",
    "      A list object returned by earthacces.search_data().\n",
    "      This contains the result granules from the Earthdata search.\n",
    "      \n",
    "    open_results : opened earthdata_results object\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    granule_df : DataFrame\n",
    "      A dataframe with a row for each granule file.\n",
    "      Each row will contain the granule ID, the download URL,\n",
    "      the datetime, the tile ID, and the band.\n",
    "    \"\"\"\n",
    "    columns = ['geometry','granule_id', 'file_url', 'tile_id', 'datetime', 'band_number']\n",
    "    granule_df = gpd.GeoDataFrame(columns=columns)\n",
    "    # Loop through each file in the granule results and accumulate attributes\n",
    "    for url in open_results:\n",
    "        file_url = str(url.url)\n",
    "        # Should probably use regular expression here\n",
    "        gran_id = file_url[73:107]\n",
    "        tile_id = file_url[81:87]\n",
    "        datetime = file_url[88:95]\n",
    "        band = file_url[-7:-4]\n",
    "        if band == 'ask':\n",
    "            band = 'Fmask'\n",
    "        # Find granule geometry based on granule ID\n",
    "        for result in earthdata_results:\n",
    "            result_attr = result['umm']\n",
    "            result_gran_id = result_attr['GranuleUR']\n",
    "            if result_gran_id == gran_id:\n",
    "                extent = result_attr['SpatialExtent']\n",
    "                geom_extent=extent['HorizontalSpatialDomain']['Geometry']['GPolygons']\n",
    "                coord_list = []\n",
    "                for item in geom_extent:\n",
    "                    poly = item['Boundary']['Points']\n",
    "                    for vertex in poly:\n",
    "                        longitude = vertex['Longitude']\n",
    "                        latitude = vertex['Latitude']\n",
    "                        coord_list.append((longitude, latitude))\n",
    "                granule_geometry = shapely.geometry.Polygon(coord_list)\n",
    "        # Append info for each file to dataframe\n",
    "        new_row = pd.DataFrame({'geometry': [granule_geometry],\n",
    "                   'granule_id': [gran_id],\n",
    "                   'file_url': [file_url],\n",
    "                   'tile_id': [tile_id],\n",
    "                   'datetime': [datetime],\n",
    "                   'band_number': [band]\n",
    "                  })\n",
    "        granule_df = pd.concat([granule_df, new_row], ignore_index=True)\n",
    "    return granule_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f458281-90e3-4673-88b2-327857ff5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cached(\"download_df\", False)\n",
    "def do_something(results, open_results):\n",
    "    download_df = create_granule_df(results, open_results)\n",
    "    return download_df\n",
    "download_df = do_something(results, open_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa3fdb-e11e-4793-88d6-85d9f80a2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute image mask\n",
    "def process_tifs(download_df, huc_gdf):\n",
    "    \"\"\"\n",
    "    Load and process the Fmask and B band .tif files\n",
    "    in a granule.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    download_df : pandas.core.groupby.generic.DataFrame\n",
    "      Dataframe containing all the rows for each band .tif for all the files.\n",
    "    \n",
    "    huc_gdf : GeoDataFrame\n",
    "      Geodataframe containing the geometry of the HUC watershed used to clip\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    accumulator_df : Pandas dataframe\n",
    "      Dataframe containing the processed rows for all the tifs.\n",
    "      An xarray has been added for each processed .tif in the last column.\n",
    "    \"\"\"\n",
    "    grouped = download_df.groupby('granule_id')\n",
    "    columns = ['geometry',\n",
    "               'granule_id', \n",
    "               'file_url', \n",
    "               'tile_id', \n",
    "               'datetime', \n",
    "               'band_number',\n",
    "               'processed_da']\n",
    "    accumulator_df = pd.DataFrame(columns=columns)\n",
    "    # Looping through each Granule and processing data\n",
    "    # Create cloud mask\n",
    "    for granule_id, group_data in grouped:\n",
    "        print(\"Processing data for Granule ID:\", granule_id)\n",
    "        granule_gdf = group_data\n",
    "        fmask_gdf = group_data['band_number'] == \"Fmask\"\n",
    "        fmask_gdf = group_data[fmask_gdf].iloc[0:1]\n",
    "        file_name = (\n",
    "            fmask_gdf.iloc[0]['granule_id'] +\n",
    "            \".\" + fmask_gdf.iloc[0]['band_number'] + '.tif'\n",
    "        )\n",
    "        file_path = data_dir + '\\\\earthpy-downloads\\\\' + file_name\n",
    "        fmask_da = rxr.open_rasterio(file_path).squeeze()\n",
    "        huc_gdf_crs = huc_gdf.to_crs(fmask_da.rio.crs)\n",
    "        fmask_crop_da = fmask_da.rio.clip_box(*huc_gdf_crs.total_bounds)\n",
    "        mask = compute_mask(fmask_crop_da)\n",
    "        # Apply crop, scale factor, and mask to all \"B\" files\n",
    "        for index, row in group_data.iterrows():\n",
    "            if (row['band_number'].startswith('B')\n",
    "                and row['band_number'] not in ('B10', 'B11')):\n",
    "                file_name = row['granule_id'] + \".\" + row['band_number'] + '.tif'\n",
    "                file_id = row['granule_id'] + \".\" + row['band_number']\n",
    "                file_path = data_dir + '\\\\earthpy-downloads\\\\' + file_name\n",
    "                bband_da = rxr.open_rasterio(file_path).squeeze()\n",
    "                huc_gdf = huc_gdf.to_crs(bband_da.rio.crs)\n",
    "                bband_crop_da = bband_da.rio.clip_box(*huc_gdf_crs.total_bounds)\n",
    "                bband_crop_da = bband_crop_da.where(bband_crop_da >= 0, np.nan)\n",
    "                scale_factor = 0.0001 \n",
    "                bband_crop_da = bband_crop_da * scale_factor\n",
    "                processed_da = bband_crop_da.where(mask)\n",
    "                add_row = pd.DataFrame({'geometry': row['geometry'],\n",
    "                    'granule_id': row['granule_id'],\n",
    "                    'file_url': row['file_url'],\n",
    "                    'tile_id': row['tile_id'],\n",
    "                    'datetime': row['datetime'],\n",
    "                    'band_number': row['band_number'],\n",
    "                    'processed_da': [processed_da]})\n",
    "                accumulator_df = pd.concat([accumulator_df, add_row])\n",
    "    return accumulator_df\n",
    "\n",
    "def compute_mask(da, mask_bits = [1,2,3]):\n",
    "    \"\"\"\n",
    "    Compute a mask layer from the Fmask\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    da : rioxarray\n",
    "      An array of the Fmask layer to use to compute the mask\n",
    "      \n",
    "    bits : list\n",
    "      A list of bits to exclude, documentation is here on page 21:\n",
    "      https://hls.gsfc.nasa.gov/wp-content/uploads/2019/01/HLS.v1.4.UserGuide_draft_ver3.1.pdf\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    mask : rioxarray\n",
    "      An array with computed mask values\n",
    "    \"\"\"\n",
    "    # Unpack bits in the Fmask array\n",
    "    # Need to reverse order of bits from most significant to little\n",
    "    bits = np.unpackbits(da.astype(np.uint8), bitorder = 'little').reshape(da.shape + (-1,))\n",
    "    \n",
    "    # Select the bits to use for the mask\n",
    "    mask = np.prod(bits[..., mask_bits]==0, axis=-1)\n",
    "    return mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a965d94-6579-4b6d-924c-acdd59b309d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run methods to process all .tifs and create accumulator dataframe\n",
    "@cached(\"accumulator_df\", False)\n",
    "def process_tifs_and_cache(download_df):\n",
    "    accumulator_df = process_tifs(download_df, huc_gdf)\n",
    "    return accumulator_df\n",
    "accumulator_df = process_tifs_and_cache(download_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9dafa6-ea9b-4245-84a7-48896465e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by 'band_number' and 'datetime'\n",
    "grouped_band_df = accumulator_df.groupby('band_number')\n",
    "\n",
    "# Iterate through each band\n",
    "band_accumulator = []\n",
    "for band_number, band_data in grouped_band_df:\n",
    "    # Iterate over each date\n",
    "    date_accumulator = []\n",
    "    for datetime, date_data in band_data.groupby('datetime'):\n",
    "        # Merge data from all 4 granules\n",
    "        merged_date_array = rxrm.merge_arrays(date_data['processed_da'].values)\n",
    "        # Add datetime dimension\n",
    "        merged_date_array = merged_date_array.assign_coords(date=datetime)\n",
    "        # Mask any negative values\n",
    "        merged_date_array = xr.where(merged_date_array <= 0, np.nan, merged_date_array)\n",
    "        # Append to accumulator list\n",
    "        date_accumulator.append(merged_date_array)\n",
    "    \n",
    "    # Concatenate the merged DataArrays along a new 'date' dimension\n",
    "    date_accumulator = xr.concat(date_accumulator, dim='datetime')\n",
    "    # Take the mean along the 'date' dimension to create a composite image\n",
    "    composite_image = date_accumulator.mean(dim='datetime', skipna=True)\n",
    "    \n",
    "    # Add the 'band_number' as a new dimension and give the DataArray a name\n",
    "    composite_image = composite_image.assign_coords(band_number=band_number)\n",
    "    #composite_image.name = str(band_number) + \"_array\"\n",
    "    #print(composite_image.name)\n",
    "    \n",
    "    band_accumulator.append(composite_image)\n",
    "\n",
    "# Concatenate along the 'band_number' dimension\n",
    "all_bands_array = xr.concat(band_accumulator, 'band_number')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932e8dc-789d-4487-a811-81862d812b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data array of bands\n",
    "all_bands_array.name = 'reflectance'\n",
    "model_df = (\n",
    "    all_bands_array\n",
    "    .sel(band_number=['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B09'])\n",
    "    .to_dataframe()\n",
    "    .reflectance\n",
    "    .unstack('band_number')\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Fit KMeans model to my data\n",
    "k_means = KMeans(n_clusters=6)\n",
    "model_df['category'] = k_means.fit_predict(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c8207-600f-4f8c-b047-6dcf862f6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to display KMeans categories\n",
    "model_array = model_df.category.to_xarray()\n",
    "model_plot = (model_df\n",
    "              .category\n",
    "              .to_xarray()\n",
    "              .sortby(['x','y'])\n",
    "              .hvplot(x='x', y='y', colormap='Colorblind', aspect=1, title='KMeans Categories')\n",
    "             )\n",
    "\n",
    "# Prepare the data to display as RGB\n",
    "rgb = all_bands_array.sel(band_number=['B04', 'B03', 'B02'])\n",
    "rgb_unint8 = (rgb * 255).astype(np.uint8())\n",
    "rgb_brighten = rgb_unint8 * 10\n",
    "rgb_plot = (\n",
    "    rgb_brighten\n",
    "    .hvplot\n",
    "    .rgb(y='y', x='x', bands='band_number', aspect=1, colormap='RGB', title='RGB Aerial Image')\n",
    ")\n",
    "\n",
    "rgb_plot + model_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c84d1-3c08-468f-8ebc-a31bb6dda7c8",
   "metadata": {},
   "source": [
    "### Unsupervised Land Cover Classification for HUC 180901020204"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
